{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39742f80-51b9-4264-896b-a53fd5fa0460",
   "metadata": {},
   "source": [
    "# Scripting a renderer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31557f88",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Mitsuba provides a flexible API that allows developing custom rendering pipelines that largely bypass the high-level interfaces and machinery used in the previous tutorials. This enables the use of the Mitsuba to rapidly prototype new and unconventional applications, while still leveraging the high performance JIT compiler and integrated ray acceleration data structures.\n",
    "\n",
    "In this example, we are going to implement an ambient occlusion renderer that mostly avoids using built-in plugins (e.g., does not use existing sensor and film interfaces). This tutorial can serve as a starting point for more advanced custom rendering methods.\n",
    "\n",
    "<div class=\"admonition important alert alert-block alert-success\">\n",
    "\n",
    "ðŸš€ **You will learn how to:**\n",
    "    \n",
    "<ul>\n",
    "  <li>Use Mitsuba as a framework to write your own rendering pipeline</li>\n",
    "  <li>Spawn a wavefront of rays in Python</li>\n",
    "  <li>Generate random numbers with the <tt>PCG32</tt> class</li>  \n",
    "  <li>Work with the <tt>Loop</tt> construct</li>\n",
    "</ul>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "211b789b-edc5-4343-8ff5-8468b127a36a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Like in the previous tutorials, we start by importing the Mitsuba and DrJit and loading a scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f2edef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mitsuba as mi\n",
    "import drjit as dr\n",
    "\n",
    "mi.set_variant('llvm_ad_rgb')\n",
    "\n",
    "scene = mi.load_file('../scenes/cbox.xml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfafe465",
   "metadata": {},
   "source": [
    "While it is possible to use Mitsuba in `scalar` mode, it is highly recommended to stick to the JIT-compiled variants of the system (i.e., `llvm` or `cuda`) for performance critical applications implemented using the Python API. In `scalar` mode, we would pay the overhead of the Python binding layer for every function call on every individual sample/ray in our simulation. Using a JIT-compiled variant largely eliminates any Python related overheads and allows the system to efficiently use the available hardware."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "085a070a-9621-4808-ab1c-670c1549f0a4",
   "metadata": {},
   "source": [
    "## Spawning rays\n",
    "\n",
    "In this tutorial, we replace large parts of Mitsuba's high-level rendering pipeline by relatively low level Python code to demonstrate the system's flexibility. We start by implementing a camera model and corresponding ray generation routine. In this experiment, we will implement a simple orthographic camera, given the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2f2ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera origin in world space\n",
    "cam_origin = mi.Point3f(0, 1, 3)\n",
    "\n",
    "# Camera view direction in world space\n",
    "cam_dir = dr.normalize(mi.Vector3f(0, -0.5, -1))\n",
    "\n",
    "# Camera width and height in world space\n",
    "cam_width  = 2.0\n",
    "cam_height = 2.0\n",
    "\n",
    "# Image pixel resolution\n",
    "image_res = [10, 10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf5bb44b-ae69-4af8-8219-99b0dc2c9e80",
   "metadata": {},
   "source": [
    "We will now spawn a whole wavefront of camera rays that can be processed all at once in a vectorized way. We first generate ray origins in the camera's local coordinate frame using `dr.meshgrid` and `dr.linspace`. These functions behave similarly to their equivalents in NumPy. We construct a 2D grid of ray origins based on the camera's physical dimensions (`cam_height`, `cam_width`) and image resolution (`image_res`).\n",
    "\n",
    "The ray origins in local coordinates then need to be transformed into world space to account for the camera's viewing direction and 3D position. We first construct a coordinate frame ([<code>mi.Frame3f</code>][1]) that is oriented in the camera's viewing direction. Using its [<code>to_world()</code>][2] method we rotate our ray origins into world space and finally add the camera's world space position.\n",
    "\n",
    "[1]: https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.Frame3f\n",
    "[2]: https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.Frame3f.to_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "380f4d14-2afb-4e88-8d2c-b572e2511a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a grid of 2D coordinates\n",
    "x, y = dr.meshgrid(\n",
    "    dr.linspace(mi.Float, -cam_width  / 2,   cam_width / 2, image_res[0]),\n",
    "    dr.linspace(mi.Float, -cam_height / 2,  cam_height / 2, image_res[1])\n",
    ")\n",
    "\n",
    "# Ray origin in local coordinates\n",
    "ray_origin_local = mi.Vector3f(x, y, 0)\n",
    "\n",
    "# Ray origin in world coordinates\n",
    "ray_origin = mi.Frame3f(cam_dir).to_world(ray_origin_local) + cam_origin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "276e3b15-b26f-45f9-a5d9-86aeaa7bab6e",
   "metadata": {},
   "source": [
    "We can now assemble a wavefront of world space rays that will later be traced in our rendering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f7eb35-b6c9-48ed-8876-1dc595d391a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray = mi.Ray3f(o=dr.slice(ray_origin,12), d=cam_dir)\n",
    "ray = mi.Ray3f(o=ray_origin, d=cam_dir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "835e6bbe-8d62-45b5-b946-ca532d2011ce",
   "metadata": {},
   "source": [
    "We then intersect those primary rays against the scene geometry to compute the corresponding surface interactions (of type [<code>SurfaceInteraction3f</code>][1]).\n",
    "\n",
    "[1]: https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.SurfaceInteraction3f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0d9f87-33ad-4724-aa4e-d277bec3ba84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "si = scene.ray_intersect(ray)\n",
    "print(dr.width(si))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23b19c52-7b1b-4613-ba0b-89bcb1dbacb2",
   "metadata": {},
   "source": [
    "## Ambient occlusion\n",
    "\n",
    "Ambient occlusion is a rendering technique that calculates the average local occlusion of surfaces. For a point on the surface, we trace a set of rays (`ambient_ray_count`) in random directions on the hemisphere and compute the fraction of rays that intersect another surface within a specific maximum range (`ambient_range`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab3a2bcc-1c7a-42d8-bb88-2049ae70874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ambient_range = 0.75\n",
    "ambient_ray_count = 256"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8c7115e-f9bd-45d5-9f0d-3bd48fabadb5",
   "metadata": {},
   "source": [
    "To sample random directions on the hemisphere, we need to instantiate a random number generator. Instead of using an existing `Sampler` plugin, we directly use the [<code>PCG32</code>][1] class that is provided by DrJit. This random number generator is initialized using the size of our wavefront of rays. We can then call [<code>rng.next_float32()</code>][2] to sample uniformly distributed random numbers in $[0, 1)$.\n",
    "\n",
    "[1]: https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.PCG32\n",
    "[2]: https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.Sampler.next_float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f048def-b943-4f22-9b6d-60428acfa781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the random number generator\n",
    "rng = mi.PCG32(size=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "064bcbfa-c816-4693-9b09-4703cca2671e",
   "metadata": {},
   "source": [
    "In the following code, we loop over ambient occlusion samples and use `mi.Loop` for performance reasons (see the [DrJIT documentation][1] for details).\n",
    "\n",
    "The loop body of this algorithm is fairly simple: \n",
    "\n",
    "1. We first draw two random numbers from the `PCG32` instance. \n",
    "2. We use those random numbers to sample directions on the hemisphere (in local coordinates, where the z-axis is aligned to the surface normal). \n",
    "3. Those directions then need to be transformed to world space using the local coordinate frame of the surface. The surface interaction record `si` ([<code>SurfaceInteraction3f</code>][2]) stores this local coordinate frame ([<code>Frame3f</code>][3]) which can be used for this transformation.\n",
    "4. We then spawn probe rays into the sampled world space direction using `si.spawn_ray(...)`. This method implements some logic to prevent self-intersection with the surface at `si`. It should always be preferred over constructing the ray manually in such situations.\n",
    "5. We set the ambient occlusion ray's `maxt` value to only find occluders in the provided maximum range.\n",
    "6. We accumulate a value of `1.0` if the ray did not intersect any scene geometry.\n",
    "7. Finally we increment the loop iteration counter and move on to the next iteration.\n",
    "\n",
    "After the loop, we divide the result by the number of ambient occlusion samples to get the average occlusion.\n",
    "\n",
    "[1]: https://drjit.readthedocs.io\n",
    "[2]: https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.SurfaceInteraction3f\n",
    "[3]: https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.Frame3f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66ec9173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop iteration counter\n",
    "i = mi.UInt32(0)\n",
    "\n",
    "# Accumulated result\n",
    "result = mi.Float(0)\n",
    "\n",
    "# Initialize the loop state (listing all variables that are modified inside the loop)\n",
    "loop = mi.Loop(name=\"\", state=lambda: (rng, i, result))\n",
    "\n",
    "while loop(si.is_valid() & (i < ambient_ray_count)):\n",
    "    # 1. Draw some random numbers\n",
    "    sample_1, sample_2 = rng.next_float32(), rng.next_float32()\n",
    "    \n",
    "    # 2. Compute directions on the hemisphere using the random numbers\n",
    "    wo_local = mi.warp.square_to_uniform_hemisphere([sample_1, sample_2])\n",
    "\n",
    "    # Alternatively, we could also sample a cosine-weighted hemisphere\n",
    "    # wo_local = mi.warp.square_to_cosine_hemisphere([sample_1, sample_2])\n",
    "    \n",
    "    # 3. Transform the sampled directions to world space\n",
    "    wo_world = si.sh_frame.to_world(wo_local)\n",
    "\n",
    "    # 4. Spawn a new ray starting at the surface interactions\n",
    "    ray_2 = si.spawn_ray(wo_world)\n",
    "    \n",
    "    # 5. Set a maximum intersection distance to only account for the close-by geometry\n",
    "    ray_2.maxt = ambient_range\n",
    "\n",
    "    # 6. Accumulate a value of 1 if not occluded (0 otherwise)\n",
    "    result[~scene.ray_test(ray_2)] += 1.0\n",
    "    \n",
    "    # 7. Increase loop iteration counter\n",
    "    i += 1\n",
    "\n",
    "# Divide the result by the number of samples\n",
    "result = result / ambient_ray_count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f2c7c4f-5412-411d-957a-c5986d9e2e2b",
   "metadata": {},
   "source": [
    "## Displaying the result\n",
    "\n",
    "The algorithm above accumulated ambient occlusion samples in a 1-dimensional array `result`. To work with this result as an image, we construct a [<code>TensorXf</code>][1] using the image resolution specified earlier. \n",
    "\n",
    "[1]: https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.TensorXf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10dc99f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.76171875, 0.87109375, 0.953125, 0.9921875, .. 90 skipped .., 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = mi.TensorXf(result, shape=image_res)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c02d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result[12]\n",
    "# 0.83203125"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf9f7155-ced4-4400-9eed-f24369c2a4ea",
   "metadata": {},
   "source": [
    "Now let's visualize our ambient occlusion rendering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d571d44-6e7c-432d-9cb2-688fe99cec74",
   "metadata": {
    "nbsphinx-thumbnail": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(image, cmap='gray'); plt.axis('off');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c41f4d95-fb05-4ee8-a51f-95d49b34b387",
   "metadata": {
    "tags": []
   },
   "source": [
    "## See also\n",
    "\n",
    "- [<code>mitsuba.Sampler</code>][1]\n",
    "- [<code>mitsuba.PCG32</code>][2]\n",
    "- [<code>mitsuba.warp.square_to_uniform_hemisphere()</code>][3]\n",
    "- [<code>mitsuba.SurfaceInteraction3f.spawn_ray()</code>][4]\n",
    "\n",
    "[1]: https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.Sampler\n",
    "[2]: https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.PCG32\n",
    "[3]: https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.warp.square_to_uniform_hemisphere\n",
    "[4]: https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.SurfaceInteraction3f.spawn_ray"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
